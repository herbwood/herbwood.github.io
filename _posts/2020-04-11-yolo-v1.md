---
layout: post
title:  "[DL] YOLO v1 논문 리뷰"
subtitle:   "yolo v1"
categories: study
tags: dl
comments: true
use_math : true
---

R-CNN 계열의 Object Detection 모델들은 객체가 있는 위치를 bounding box를 통해 예측하고 범주를 예측하는 과정이 분리된 과정을 통해 수행됩니다. 이러한 이유로 R-CNN 계열의 모델은 two-stage detector라고 불립니다. 이러한 two-stage detector의 경우 탐지 성능은 뛰어나지만 속도가 느리다는 단점이 있습니다. 이번 포스팅에서는 detection 속도 문제를 해결한 최초의 one stage detctor인 YOLO v1 모델에 대해 살펴보도록 하겠습니다. 

### What's the Problem?

앞서 언급했다시피 R-CNN의 계열의 모델은 객체 탐지 시 bounding box 예측과 범주 예측 과정이 분리되어 있습니다. 이러한 문제를 해결하기 위해 Faster R-CNN 모델에서는 RoI Pooling layer와 multi-task loss를 통해 여러 컴포넌트로 구분된 stage를 하나의 네트워크에 편입시켰습니다. 이를 통해 어느 정도 속도 향상이 있었으나 각각의 컴포넌트를 개별적으로 학습시켜야 하기 때문에 학습 속도가 느릴 수밖에 없습니다. 

### Improvements

YOLO는 object detection 문제를 bounding box 예측과 범주 분류 과정을 하나의 문제로 정의합니다. 직관적으로 보면 이미지 내의 객체에 대한 위치를 예측하고 범주를 예측하는 두 번의 과정이 아니라 한번에 객체의 위치와 범주를 예측한다는 것입니다. 이는 **"you only look once (YOLO) at an image to predict what objects are present and where they are"** 라는 논문 내의 한 문장에서 잘 드러납니다. 

- R-CNN 모델과 같이 복잡한 pipeline이 필요하지 않는 **Unfied Network**를 통해 학습하기 때문에 탐지 속도가 빠릅니다. Titan X GPU 사용시 초당 45 프레임이 나옵니다. 이는 실시간 객체 탐지가 가능하다는 것을 의미합니다. 

- 또한 이미지를 전역적으로 바라본다는 장점이 있습니다. YOLO는 R-CNN 모델이 이미지를 부분부분 학습했던 것과 달리 이미지 한 장을 **전역적(globally)**으로 바라봅니다. 이는 뒤에서도 언급하겠지만 이미지를 전역적인 특징을 학습함으로써 배경을 객체로 예측하는 실수가 R-CNN 모델에 비해 상대적으로 적습니다. 

### Model Architecture

#### Training

<p align='center'><img src='https://whal.eu/i/0p2w4P27'></p><p align='center'>[그림 1] YOLO 학습 과정</p>


1) 먼저 이미지를 448x448 크기로 resize시켜줍니다

2) 그 다음 이미지를 7x7 크기로 나눠줍니다. 이 때 나눠진 각 영역을 **grid cell**이라고 불립니다. 총 49개의 grid cell이 생깁니다. 각 grid cell은

1) bounding box의 좌표와 confidence(예측된 box와 ground truth 사이의 IOU값)
2) Pr(class|object) C는 class의 조건부 확률

만약 Object의 중심(object의 가운데 좌표)이 특정 grid cell에 위치한다면 해당 grid cell은 그 object를 찾는 책임이 주어집니다(학습을 통해 해당 object를 찾는 것이 목표가 된다는 의미입니다). 


<p align='center'><img src='https://whal.eu/i/znPXMABn'></p><p align='center'>[그림 2] YOLO 전체 네트워크</p>

3) convolution layer가 이미지로부터 feature를 추출하면 fc layer는 output 확률과 좌표를 예측합니다. YOLO는 GoogLeNet 모델로부터 영감을 받아 24개의 conv layer와 2개의 fully-connected layer를 가집니다. 1x1 conv layer와 3x3 conv layer를 돌아가면서 적용하고 maxpooling 대신에 strided convolution을 사용했습니다. 이러한 네트워크를 거쳐 최종 출력값은 7x7 x 24(21개의 범주 + 4개의 bounding box 좌표) 형태를 가집니다. 

#### Loss function

- YOLO 모델은 loss function으로 **sum-squared error**를 사용합니다. 이 loss function은최적화하기 쉽지만 bounding box의 위치 error와 classfication error의 가중치가 같다는 문제가 있습니다. 이를 해결하기 위해서 논문의 저자는 **scaling factor lambda**를 사용하여 localization error와 classification error의 가중치를 조정합니다. 최종 모델에서 lamba=4를 사용합니다. 

- 또 다른 문제는 sum-squared error는 큰 bounding box와 작은 bounding box의 error를 동등하게 여길 가능성이 있다는 것입니다. 큰 bounding box의 작은 에러는 작은 bounding box의 에러보다 덜 심각합니다. 논문의 저자는 이를 해결하기 위해 bounding box의 넓이와 높이에 루트를 씌워줍니다. 

- 또한 

<p align='center'><img src='https://whal.eu/i/JExxbZWE' width='400px'></p><p align='center'>[그림 3] YOLO의 loss function</p>

만약 cell i가 클래스별 소속 확률인 class probability pi, ...와 bounding box의 좌표정보인 x, y, w, h를 예측했을 때 YOLO의 전체 loss function은 위와 같습니다. 가장 앞에 있는 변수는 object가 cell i에 있는지 여부로 encoding해줍니다. 만약 object가 cell i에 없으면 0, 있으면 1 값을 가집니다. 기존 R-CNN 게열의 모델과 같이 여러 개의 loss function을 사용하는 것이 아닌 하나의 regression loss로 loss function을 정의한 것을 확인할 수 있습니다. 

#### Parameterizeing Class Probabilities

- 각 grid cell은 범주별 소속확률을 예측합니다. 한 장의 이미지당 49개의 grid cell이 있으며 각 20개의 범주에 대해 예측하므로 980개의 예측된 확률이 도출된다. 하지만 이미지 내에 매우 적은 수의 객체만 등장할 것이기 때문에 대부분의 확률은 0일 것이다. 

- 이러한 불균형한 결과값으로 인해 학습 중 loss function은 발산할 가능성이 있습니다. 이러한 문제를 해결하기 위해 논문의 저자는 각 grid location에 **extra variable**을 추가한다. 그것은 범주와 상관없이 어떤 객체가 존재할 확률입니다. 즉 논문에서는 20개의 범주에 더해 1개의 **objectness 확률**을 추가합니다. 따라서 grid cell은 Pr(Object)와 20개의 조건 확률을 예측하게 됩니다. 이를 통해 prob가 0으로 학습되는 경우는 크게 줄어들게 됩니다.  

### Experiments

<p align='center'><img src='https://whal.eu/i/JExxOrVE' width='500px'></p><p align='center'>[그림 3] YOLO v1과 다른 detection 모델과 mAP값 비교</p>

VOC 2012 데이터셋을 사용할 경우 YOLO v1은 54.5 mAP값을 가집니다. 이는 최신 기법에 비해 성능이 떨어지나 몇몇 범주에 대해서는 다른 모델보다 높은 예측 확률을 보입니다.

<p align='center'><img src='https://whal.eu/i/bnNYmZR7' width='500px'></p><p align='center'>[그림 3] YOLO v1과 다른 detection 모델과 속도 비교</p>

테스트 시 YOLO는 초당 45프레임을 처리합니다. 이는 R-CNN에 비해 400~500배 빠른 속도이며 Fast R-CNN에 비해 100배 이상 빠릅니다. 

<p align='center'><img src='https://whal.eu/i/opqdLR5E' width='300px'></p><p align='center'>[그림 3] YOLO v1과 Fast R-CNN과 배경 error 정도 비교</p>

- 객체 탐지 방법은 높은 성능을 위해 높은 재현율(recall)을 가져야합니다. 하지만 YOLO v1 모델은 **높은 공각 제약(spatial restriction)**을 두기 대문에 서로의 위치가 가까운 작은 객체에 대한 탐지가 어렵다는 문제가 있습니다. 이러한 가정 하에 재현율은 93.1%로 Selective Search가 98.0%인 것에 떨어지나 비교적 나쁘지 않습니다. 

- Fast R-CNN과 비교할 경우 localization error = 13.6%, background error=8.6인 반면, YOLO v1은 각각 24.7%, 4.3%의 값을 가집니다. R-CNN 계열의 모델은 이미지를 지역적으로 바라보기 때문에 전체 맥락에서 봤을 때 객체가 아님에도 객체라고 탐지하는 경향이 있습니다. 논문의 저자는 두 모델을 결합하여 Fast R-CNN + YOLO를 통해 각 모델의 단점을 해소하고자 했고 실제로 당시 결합한 모델을 통해 VOC 2012 데이터셋을 통해 2등을 차지했습니다.

### Conclusion

YOLO v1은 기존의 객체 탐지 방식과 다른 접근 방법을 통해 추론 속도 측면에서 좋은 모습을 보였습니다. 하지만 논문에서도 언급되어 있지만 YOLO v1에는 큰 문제점이 있습니다. 하나의 gride cell은 오직 하나의 bounding box만을 예측하기 때문에 새 떼와 같은, 하나의 grid cell에 있는 여러 object를 예측하기 어렵다는 문제가 있습니다. 이러한 YOLO v1의 문제점을 해결하고 등장한 모델이 바로 SSD(Single Shot Detection)모델입니다. 다음 포스팅에서는 SSD 모델에 대해 살펴보도록 하겠습니다. 

### Reference

[YOLO v1 논문](https://arxiv.org/pdf/1506.02640.pdf)  
[YOLO 모델에 대해 잘 설명한 PPT](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.g137784ab86_4_1079)  
[Strided convolution에 대해 설명한 영상](https://www.coursera.org/lecture/convolutional-neural-networks/strided-convolutions-wfUhx)  