---
layout: post
title:  "[ML-12]Adaboost"
subtitle:   "how to adaboost"
categories: study
tags: ml
comments: false
use_math : true
---

## 12. Adaboost

### what?
- 약한 분류기들을 순차적으로 학습하여 이를 결합하여 성능이 높은 강한 분류기를 반환하는 알고리즘
- 약한 분류기는 학습할 때 이전 분류기가 잘못 분류한 데이터에 가중치를 주어 다음 분류기에서 올바르게 분류하도록 한다. 
- adaboost에 대해 알아야 할 점들
1) 약한 분류기를 결합하여 사용한다  
2) 어떤 분류기는 다른 분류기보다 더 가중치가 높다  
3) 분류기를 학습시키는 순서는 중요하다(이전 분류기의 결과가 다음 분류기에 영향을 주기 때문에)  

![Cap 2019-10-03 17-55-08-090](https://user-images.githubusercontent.com/35513025/66113039-f40a0380-e606-11e9-93ba-5095b0169964.jpg)


### why?
- 실행하기 쉽고(파라미터가 T뿐..) 빠르다
- 유연성이 높아 다른 머신러닝 모델과도 사용하기 좋다

### why not?
- 잘못 분류된 샘플에 집중하여 학습 데이터에 의존하여 overfitting되기 쉽다. 이는 boosting 알고리즘에서 공통적으로 나타나는 문제점이다. 
- 이전 분류기의 학습 결과가 다음 분류기로 전파되기 때문에 모든 데이터에 공통적으로 나타나는 노이즈에 취약하다

### how?
#### ```Input``` 
1)Data{($$x_i, y_i$$)}, M rows(data) and 1 column(feature)   
2) iteration round T : 몇 개의 약한 분류기를 조합하여 하나의 강한 분류기를 만들지 반복 횟수
#### ```step 1``` Initialize weights of training samples
1) positive samples의 수가 P개 : positive samples의 initial weight = $\frac 1 P$  
2) negative samples의 수가 F개 : negative samples의 initial weight = $\frac 1 F$  
#### ```step 2```for t = 1 to T
1) 모든 feature에 대해 training sample을 얼마나 잘 분류하는지 성능 평가 하여 각 feature마다 total error를 구한다. ***(total error) : $E$ =  (misclassified samples) / (total samples)***  
2) 분류 성능이 가장 좋은(total error가 가장 적은) 하나의 feature를 해당 round t의 (약한)분류기로 선정한다.  
3) 선정된 약한 분류기의 중요도를 구한다.   
***amount of say : $S = \frac 1 2 log(\frac {1-E} E)$***
4) 데이터의 가중치를 업데이트 한다. 
4-1) Increase incorrectly classified samples  
(New sample weight) = (sample weight) * $e^S$
4-2) Decrease correctly classified samples  
(New sample weight) = (sample weight) * ${\frac 1 e}^S$
5) 데이터 가중치의 합이 1이 되도록 가중치를 normalize해준다
#### ```step 3``` output : strong classifier
- T개의 약한 분류기를 weighted linear combination하여 최종 강한 분류기를 얻는다.
- 강한 분류기 $H(x) = S_1h_1(x) + S_2h_2(x)+...+S_Th_T(x) = \sum_{t=i^T}S_th_t(x)$  
H = 최종 강한 분류기  
h = 약한 분류기  
S = 약한 분류기의 중요도  
t = iteration round(1,2,...,T)

### Code usage
```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm='SAMME.R', learning_rate=0.5, random_state=42
)
ada_clf.fit(X_train, y_train)
```
### Reference 
[핸즈온 머신러닝](https://github.com/rickiepark/handson-ml)      
[adaboost 설명](https://dic1224.blog.me/220989033563)  
[adaboost 설명 유튜브 영상(반드시 보는 걸 추천!)](https://www.youtube.com/watch?v=LsK-xG1cLYA)  



