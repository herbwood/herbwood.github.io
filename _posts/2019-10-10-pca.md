---
layout: post
title:  "[ML-18]PCA(Principal Component Analysis)"
subtitle:   "how to pca"
categories: study
tags: ml
comments: true
use_math : true
---

## 18. PCA(Principal Component Analysis)

### what?
- 차원의 데이터를 저차원의 데이터로 환원시키는 기법이다. 서로 연관 가능성이 있는 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간(주성분)의 표본으로 변환하기 위해 직교 변환을 사용한다. 
- 데이터의 차원을 축소하는 대표적인 알고리즘
![pca](https://skymind.ai/images/wiki/scatterplot_line.png)

### why?
- 데이터에 대한 직관을 잘 반영한다.
- 데이터의 크기를 극적으로 줄여줘 데이터를 가공하는데 시간이 절약되고 더 적은 저장공간을 필요로 한다

### why not?
- 데이터가 선형관계를 가지고 있지 않다면 효과가 좋지 않다. 
- 결과를 이해하기 어렵고 시각화하기 어렵다
- 

### how?
#### ```Input```  
1) Data ${(x_i, y_i)_{i=1}^M}$, M samples and N columns
2) k : dimensions to reduce

#### ```Step 1``` Scale the Data
for i = 1 to m, $x_i \leftarrow \frac {x_i - \mu} {\sigma}$
- 벡터를 이루는 데이터간의 sclae을 맞추기 위해 data scaling을 진행한다. 

#### ```Step 2``` 
1) Covariance Matrix $\sum$ 구하기 
- 두 feature 간의 선형 의존성을 나타내는 지표이다
- $\sum = \frac 1 m \sum_{i=1}^M(x^{(i)})(x^{(i)})^T$

2) SVD(Singular Vector Decomposition)을 통해 $\sum$의 eigenvector(고유벡터) 구하기
- eigenvector(고유벡터) : 0이 아닌 값을 가지면서 선형변형이 일어난 후에도 방향이 바뀌지 않는 벡터

3) U matrix를 크기 순(자료의 변화량이 가장 큰 순)대로 고유값과 고유벡터를 정렬하여 $U_{reduced}$ matrix를 얻는다
- $U$ matrix : (n x n) -> $U_{reduced}$ : (n x k)

4) $z = U_{reduced}$ x $x^{(i)}$

#### ```Step 3``` 차원이 축소된 matrix $z$


### Code usage
```python
import numpy as np
from sklearn.decomposition import PCA

# make 3 dimensinal data
np.random.seed(4)
m = 60
w1, w2 = 0.1, 0.3
noise = 0.1

angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5
X = np.empty((m, 3))
X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m)/2
X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m)/2
X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```
### Reference 
[핸즈온 머신러닝](https://github.com/rickiepark/handson-ml)      
[Coursera : Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning/home/welcome)


