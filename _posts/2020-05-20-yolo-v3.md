---
layout: post
title:  "[DL] YOLO v3 논문 리뷰 "
subtitle:   "yolo v3"
categories: study
tags: dl
comments: true
use_math : true
---

이번 포스팅에서는 YOLO v3 모델에 대해 살펴보도록 하겠습니다. YOLO v2가 이전 버전에 비해 혁신적인 변화를 주었다면 v3 모델은 v2를 더 나은 방향으로 세말하게 수정하는 식으로 개선되었습니다. ~~논문도 짧아서 읽기 좋았습니다~~따라서 YOLO v3가 v2를 어떤 식으로 조정했는지를 중심으로 살펴보도록 하겠습니다. 


### Improvements

#### Bounding Box Prediction
- YOLO v3는 각 bounding box마다 logistic regression을 사용하여 **objectness score**(객체의 존재 확률) 예측을 추가합니다. 예를 들어 bounding box가 ground truth와 완전히 겹쳐진다면 1이라는 값을 반환하게 됩니다. 

- 또한 Faster R-CNN의 **Matching Strategy**를 활용합니다. 하나의 bounding box에 ground truth를 할당하여 IOU threshold를 넘으면 positive, 넘지 못하면 negative로 labeling합니다(이후 label은 loss function에 활용됩니다). threshold 사이의 값을 가진 bounding box에 대해서는 loss function 계산 시 objectness score에 대해서만 loss를 구합니다. 

#### Class Prediction

YOLO v2는 WordTree를 활용하여 Multi label(예를 들어 노포크 테리어-사냥개-개)을 가집니다. 논문의 저자는 범주 에측을 위해 softmax 함수를 사용할 경우 하나의 bounding box가 하나의 label을 가정하여 multilabel dataset을 활용하는 YOLO에 적합하지 않다고 봤습니다. 따라서 YOLO v3부터는 범주 예측을 위해 softmax 함수가 아닌 **independent logistic classfier**를 사용합니다. 이에 맞춰 loss function 역시 binary cross entropy로 바꿉니다. 

#### Prediction Across Scales

YOLO v3는 Feature Pyramid Network와 유사하게 3가지 다른 scale의 box를 예측합니다. 특징 추출기인 darknet-19에 몇 개의 conv layer만 추가하여 다양한 scale의 feature를 추출할 수 있습니다. 최종적으로 예측하는 값의 형태는 **N x N x [3 x (4+1+80)]** 입니다. N은 grid의 크기, 3은 anchor box의 수, 4는 offset, 1은 objectness score, 80은 범주의 수를 의미합니다. 

- 최종 출력단 2개 전의 layer에서 feature map을 가져와 2배로 upsample해주고 바로 이전 layer의 feature map을 가져와 두 feature map을 합쳐줍니다. 이를 통해 upsampled된 feature map에서는 보다 전반적인 의미를 담은 정보를, 바로 이전 feature map에서는 보다 세밀한(fine-grained) 정보를 모두 얻을 수 있습니다. 

- 위와 같은 과정을 한 번 더 거쳐 bounding box를 예측합니다. 
- 3개의 anchor box를 3개의 scale에 적용하여 총 9개의 anchor box를  k-means clustering을 통해 얻습니다. 

#### Feature Extractor

<p align="center"><img src="https://user-images.githubusercontent.com/24144491/48928080-f4356f00-ef1e-11e8-9467-40dee0e31bf6.JPG" width="400px"></p>
<p align="center">[그림 1]Darknet-53</p>

YOLO v3는 새로운 feature extractor를 선보입니다. 3x3 conv layer와 1x1 conv layer를 반복적으로 추가하였으며, ResNet에서 비롯된 skip connection을 적용하여 이전보다 깊은 네트워크인 **Darknet-53**를 설계했습니다.  DarkNet-53은 ResNet-152와 비슷한 정확도를 가지나, 추론 속도가 2배 이상 빠른 높은 성능을 보입니다. 

### Performance

<p align="center"><img src="https://ifh.cc/g/oGjD2h.png"></p>
<p align="center">[그림 2]YOLO v3과 다른 모델과의 성능 비교</p>

<p align="center"><img src="https://t1.daumcdn.net/cfile/tistory/9972EB385B2DEDEF33" width="500px"></p>
<p align="center">[그림 3]YOLO v3와 RetinaNet 비교</p>

- YOLO v3의 AP값은 SSD와 비슷하지만 3배 이상 빠릅니다. 하지만 RetinaNet과 같은 최신 기법들에 비해 성능이 살짝 떨어지는 모습을 보입니다. 
- 이전보다 IOU threshold를 늘리면서 YOLO v3는 bounding box를 완벽하게 찾는 과정에서 약간의 성능 하락이 있습니다. 예전에 YOLO는 작은 객체를 찾지 못하는 문제가 있었지만 현재는 반대의 문제를 겪고 있습니다. 오히려 중간이나 큰 크기의 객체를 제대로 찾지 못합니다. 

### Conclusion

YOLO v3 논문은 논문이라기 보다 블로그를 보는 생각이 들었습니다. 어떻해서든지 개발한 모델의 장점만을 부각하기보다 논문의 저자들이 시도했다가 실패한 내용을 싣거나, 최신 기법인 RetinaNet보다 성능 면에서 떨어진다는 점을 시인했다는 점에서 그런 느낌이 들었습니다. 심지어 컴퓨터 비전 분야가 가져올 수 있는 윤리적 책임을 부각하는 결론 부분은 특히 인상적이었습니다(+트위터 관뒀으니 해쉬태그 붙이지 말라는 부분도...). 

### Reference
[YOLO v3 논문](https://arxiv.org/pdf/1804.02767.pdf)  
[YOLO v3에 대해 잘 정리한 TAEU님의 블로그](https://taeu.github.io/paper/deeplearning-paper-yolov3/)    
